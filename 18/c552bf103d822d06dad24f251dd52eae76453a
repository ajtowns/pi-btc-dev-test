From: jtimon@jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Sat, 14 Nov 2015 14:48:04 +0100
Subject: [bitcoin-dev] How to evaluate block size increase suggestions.
In-Reply-To: <CADZB0_anho4c_qBq0yWoNHkMkkPgFEvQvWn8pCPkU+qS5ecS7A@mail.gmail.com>
References: <CAPkFh0s-o6BXAEC-s9s1UmFwVfMFQKStoJaM0u2Lct9yiP5QBQ@mail.gmail.com>
	<CADZB0_anho4c_qBq0yWoNHkMkkPgFEvQvWn8pCPkU+qS5ecS7A@mail.gmail.com>
Message-ID: <CABm2gDqcGD0AfRhsC70NSgZFJ4-o+xXFPGxPhVoG00OzUKYofg@mail.gmail.com>

I agree with the usefulness of at least trying to define a formal set
of criteria.
I'm afraid that with proposals that schedule future increases to the
blocksize consensus maximum or leave it for miners to decide (like
BIP100, BIP101 and BIP103) cannot be evaluated without making
assumptions about the future (or what miners will decide in the
future).
Since limiting resource consumption and mining centralization dynamics
are the reasons to have blocksize consensus maximum in the first
place, I think it would be ideal to have some simulation +
benchmarking software that is able to analyze a given proposal, give
resource consumption benchmark data about average and worst cases, and
also give some kind of metric from "mining centralization dynamics
simulations".
We could start with just a metric for concrete block sizes (for
arbitrary maximum blocksizes testchains see #6382).
Note that this is unrelated to the deployment mechanism, proposed
activation date and other details.

On Fri, Nov 13, 2015 at 5:52 PM, Angel Leon via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> For instance, a goal could be that no user on the network should wait more
> than an hour to get 3 confirmations on the blockchain so that they can
> actually have useful Bitcoins.

That depends on block space demand on a particular moment in time, the
fee paid by the example user and local relay and mining policies in
the network (and how they treat transactions with the specific form of
the transaction example) and even the network topology.
There's no consensus rule that can guarantee that all transaction from
all users will be included at most 3 blocks after they are relayed.
For starters, any user can create infinite transactions (without fee)
for free while the network will never have infinite computing
resources.
What we have is a fee estimator that observes the chain and can
estimate the market situation to tell you the average number of blocks
for a given transaction with a given feerate. I know that number was
only 14 blocks or so for free (not a single satoshi in fees)
transactions, but that has probably changed with the recent attacks...
